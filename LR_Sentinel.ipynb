{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorettarehm/AIML/blob/main/LR_Sentinel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBAoANZPfqrD"
      },
      "source": [
        "# Customer First: Automated FCA Compliance Audit\n",
        "\n",
        "This notebook implements the core functionality of the **Customer First** (Text & Policy Engine). The system uses the **LLM-as-a-Judge pattern** to audit content using regulations as guidelines, proactively detecting points where the company can improve its processes, products, services or communications to optimal adherence to regulations, in this example the Consumer Duty (FCA).\n",
        "\n",
        "## Key Features:\n",
        "1. **RAG Implementation:** FCA rulebooks (The Regulatory Constitution) are loaded as a Knowledge Base for **Semantic Retrieval**\n",
        "2. **Constitutional AI Pattern:** The judge uses explicit criteria, Chain-of-Thought (CoT), and structured output to achieve regulatory-grade reliability.\n",
        "3. **Persona Simulation:** The Judge evaluates comprehension from the perspective of diverse customer personas.\n",
        "\n",
        "#### Required packages: `openai`, `langchain-openai`, `python-dotenv`, `numpy`, `scikit-learn`, `pyPDF`"
      ],
      "id": "wBAoANZPfqrD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install openai langchain-openai python-dotenv numpy scikit-learn PyPDF2 PyCryptodome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GjHOO7e9h074",
        "outputId": "8c3c87b3-966f-457f-be8e-d4eda7560873"
      },
      "id": "GjHOO7e9h074",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting PyCryptodome\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.1.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (0.4.47)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-openai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-1.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2, PyCryptodome, langchain-openai\n",
            "Successfully installed PyCryptodome-3.23.0 PyPDF2-3.0.1 langchain-openai-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7Lcy7NyfqrH",
        "outputId": "2504c851-5a06-42d0-b96f-7b96916306c0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Libraries imported and API Key loaded.\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "# Required LangChain components\n",
        "from sklearn.metrics.pairwise import cosine_similarity # For Semantic Retrieval\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "# OPENAI_API_URL = os.getenv(\"OPENAI_API_URL\") # Only Azure setup\n",
        "DEFAULT_MODEL = \"gpt-4o-mini\" # Needed for judging\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "print(\"✅ Libraries imported and API Key loaded.\")"
      ],
      "id": "n7Lcy7NyfqrH"
    },
    {
      "cell_type": "code",
      "source": [
        "# create RAG with text extracted from FCA Consumer Duty\n",
        "FCA_KNOWLEDGE_BASE = [\n",
        "    \"FCA CONC 3.2.1: Credit agreements must clearly state the Representative APR (Annual Percentage Rate) with equal prominence to any simple interest rate advertised\",\n",
        "    \"Consumer Duty Outcome 2 (Products and Services): All products must be designed to meet the needs of identified consumers.\",\n",
        "    \"FCA BCOBS 2.2.3: Risk warnings must be easily located and displayed using a font size no smaller than the main body text (prominence rules).\",\n",
        "    \"Consumer Duty Outcome 4 (Consumer Understanding): Communication must be tailored to the target audience, ensuring clarity and avoiding jargon.\",\n",
        "    \"FCA CONC 4.5.1: Information on missed payment fees and charges must be presented clearly, ideally using a simple table or bullet points.\"\n",
        "]"
      ],
      "metadata": {
        "id": "ySW5c5TMvVFb"
      },
      "id": "ySW5c5TMvVFb",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QtLysezEfqrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ffe2762-656a-491d-e030-437dd290ba9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Customer First function defined.\n"
          ]
        }
      ],
      "source": [
        "# LLM-as-a-Judge Core Function\n",
        "def run_regulatory_judge(\n",
        "    artifact_text: str,\n",
        "    evaluation_query: str,\n",
        "    persona_description: str\n",
        ") -> dict:\n",
        "\n",
        "    # Step 1 & 2: RAG Retrieval - Find relevant FCA rules based on the query\n",
        "    retrieved_rules = find_semantically_similar_documents(\n",
        "        evaluation_query, FCA_KNOWLEDGE_BASE, top_k=2\n",
        "    )\n",
        "    context_rules = \"\\n\".join(retrieved_rules)\n",
        "\n",
        "    # Step 3: Construct the Judge Prompt with RAG and Persona Simulation\n",
        "    system_prompt = f\"\"\"\n",
        "    You are the **Customer First** and **{persona_description} Persona**.\n",
        "    Your task is to evaluate a piece of bank communication against the provided FCA Regulatory Rules.\n",
        "\n",
        "    **INSTRUCTIONS:**\n",
        "    1. **Context Check (RAG):** Analyze the relevance of the [REGULATORY CONTEXT].\n",
        "    2. **Chain-of-Thought (CoT):** First, provide step-by-step reasoning on whether the [ARTIFACT TEXT] complies with the relevant rule(s) from the context, specifically concerning the user query.\n",
        "    3. **Persona Evaluation:** From the perspective of the **{persona_description} Persona**, attempt to answer the comprehension question based ONLY on the [ARTIFACT TEXT] [24].\n",
        "    4. **Output:** Return your final assessment and score in the required JSON format.\n",
        "\n",
        "    **CRITERIA:**\n",
        "    - Compliance Score (1-5, 5=Fully Compliant/Clear)\n",
        "    - Clarity Score for Persona (1-5, 5=Perfectly Understood)\n",
        "    - Pass/Fail (Binary: PASS if Compliance >= 4 AND Clarity >= 4)\n",
        "    - Reasoning Trace (The audit log for CoT) [9]\n",
        "    \"\"\"\n",
        "\n",
        "    user_content = f\"\"\"\n",
        "    [REGULATORY CONTEXT]:\n",
        "{context_rules}\n",
        "\n",
        "    [ARTIFACT TEXT]:\n",
        "{artifact_text}\n",
        "\n",
        "    [COMPREHENSION QUESTION]: {evaluation_query}\n",
        "\n",
        "    Respond ONLY with a valid JSON object following this structure:\n",
        "    {{\"compliance_score\": \"X/5\", \"clarity_score_persona\": \"X/5\", \"pass_fail\": \"PASS/FAIL\", \"reasoning_trace\": \"[Your detailed CoT reasoning]\"}}\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the LLM Judge (forcing JSON output) [25, 26]\n",
        "    response_text = call_llm(\n",
        "        prompt=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_content}],\n",
        "        response_format={ \"type\": \"json_object\" }\n",
        "    )\n",
        "\n",
        "    # Step 4: Output Parsing\n",
        "    try:\n",
        "        return json.loads(response_text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Warning: LLM did not return valid JSON. Returning raw text.\")\n",
        "        return {\"error\": \"Invalid JSON from model\", \"raw_output\": response_text}\n",
        "\n",
        "print(\"✅ Customer First function defined.\")"
      ],
      "id": "QtLysezEfqrI"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cK7UFbUufqrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0edde1b-2c82-4c39-9181-f8f90769dceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LLM connection and embedding functions defined.\n"
          ]
        }
      ],
      "source": [
        "# Standard OpenAI implementation\n",
        "\n",
        "def personal_function(prompt, **kwargs):\n",
        "    chat_model = ChatOpenAI(\n",
        "        model=\"gpt-4o\",\n",
        "        api_key=OPENAI_API_KEY,\n",
        "        timeout=30,\n",
        "        **kwargs,\n",
        "    )\n",
        "    # LLM-as-a-Judge needs GPT-4o\n",
        "    if isinstance(prompt, list):\n",
        "        return chat_model.invoke(prompt).content\n",
        "    else:\n",
        "        return chat_model.invoke([{\"role\":\"user\", \"content\": prompt}]).content\n",
        "\n",
        "def call_llm(prompt, **kwargs):\n",
        "    # Use the personal function for simplicity\n",
        "    return personal_function(prompt, **kwargs)\n",
        "\n",
        "def call_embeddings(text):\n",
        "    # Wrapper for embedding (RAG)\n",
        "    response = client.embeddings.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-3-small\"\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "print(\"✅ LLM connection and embedding functions defined.\")"
      ],
      "id": "cK7UFbUufqrI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14782b06",
        "outputId": "18057074-08b4-455a-fb33-266cd5a86177"
      },
      "source": [
        "def find_semantically_similar_documents(query_text: str, documents: list[str], top_k: int = 1) -> list[str]:\n",
        "    # Get embedding for the query\n",
        "    query_embedding = call_embeddings(query_text)\n",
        "\n",
        "    # Get embeddings for all documents\n",
        "    document_embeddings = [call_embeddings(doc) for doc in documents]\n",
        "\n",
        "    # Calculate cosine similarity between the query and each document\n",
        "    similarities = cosine_similarity(np.array([query_embedding]), np.array(document_embeddings))[0]\n",
        "\n",
        "    # Get the indices of the top_k most similar documents\n",
        "    top_k_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "    # Return the top_k documents\n",
        "    return [documents[i] for i in top_k_indices]\n",
        "\n",
        "print(\"✅ Semantic similarity function defined.\")"
      ],
      "id": "14782b06",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Semantic similarity function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SKOv-xuXfqrJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "3b3e4486-f206-424d-a883-8e7ef1f994a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Audit for Persona: Low Literacy and Financial Anxiety\n",
            "--------------------------------------------------\n",
            "Artifact Text: **Standard APR 25.9% (variable).** The APR applied to this account is 27.5% Representative (variable).Fee for missing a payment: Please consult the complex fee matrix table in Appendix C, row 4, column B.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-202460636.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m audit_results = run_regulatory_judge(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0martifact_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mARTIFACT_SNIPPET\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mevaluation_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJUDGE_QUERY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2490697330.py\u001b[0m in \u001b[0;36mrun_regulatory_judge\u001b[0;34m(artifact_text, evaluation_query, persona_description)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Step 1 & 2: RAG Retrieval - Find relevant FCA rules based on the query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     retrieved_rules = find_semantically_similar_documents(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mevaluation_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFCA_KNOWLEDGE_BASE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     )\n",
            "\u001b[0;32m/tmp/ipython-input-3510322153.py\u001b[0m in \u001b[0;36mfind_semantically_similar_documents\u001b[0;34m(query_text, documents, top_k)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_semantically_similar_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Get embedding for the query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Get embeddings for all documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1328895381.py\u001b[0m in \u001b[0;36mcall_embeddings\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcall_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Wrapper for embedding (RAG)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     response = client.embeddings.create(\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-embedding-3-small\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;34m\"/embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_create_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingCreateParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "# 5. Simulation: Auditing a Credit Card Brochure Snippet\n",
        "\n",
        "ARTIFACT_SNIPPET = (\n",
        "    \"**Standard APR 25.9% (variable).** The APR applied to this account is 27.5% Representative (variable).\"\n",
        "    \"Fee for missing a payment: Please consult the complex fee matrix table in Appendix C, row 4, column B.\"\n",
        ")\n",
        "\n",
        "JUDGE_QUERY = \"How much will you pay if you miss a payment, and is the Representative APR clear?\" # Combines compliance check and comprehension question [24]\n",
        "\n",
        "VULNERABLE_PERSONA = \"Low Literacy and Financial Anxiety\" # [8, 27]\n",
        "\n",
        "\n",
        "print(\"Running Audit for Persona:\", VULNERABLE_PERSONA)\n",
        "print(\"--------------------------------------------------\")\n",
        "print(\"Artifact Text:\", ARTIFACT_SNIPPET)\n",
        "print(\"\\n\")\n",
        "\n",
        "audit_results = run_regulatory_judge(\n",
        "    artifact_text=ARTIFACT_SNIPPET,\n",
        "    evaluation_query=JUDGE_QUERY,\n",
        "    persona_description=VULNERABLE_PERSONA\n",
        ")\n",
        "\n",
        "print(json.dumps(audit_results, indent=4))"
      ],
      "id": "SKOv-xuXfqrJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tswjDJ--fqrJ"
      },
      "source": [
        "## Interpreting the Output (Compliance Heatmap)\n",
        "\n",
        "The output provides a structured verdict from the LLM Judge.\n",
        "\n",
        "If the 'Clarity Score for Persona' is low (e.g., due to difficulty navigating a 'complex fee matrix'), the resulting `pass_fail` should be 'FAIL'. The `reasoning_trace` acts as the mandatory audit log (Chain-of-Thought) explaining the verdict.\n",
        "\n",
        "This approach automates the audit of static content, helping the compliance team identify areas of confusion or non-compliance."
      ],
      "id": "tswjDJ--fqrJ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}