{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorettarehm/AIML/blob/main/LR_Capstone_SalesForecasting_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Sales Forecasting with Time Series Analysis\n",
        "\n",
        "## Project Overview\n",
        "This notebook provides a comprehensive analysis and forecasting solution for restaurant sales data using advanced time series techniques. The project demonstrates:\n",
        "\n",
        "- **Objective**: Predict future sales using historical transaction data\n",
        "- **Scope**: Multi-restaurant, multi-item sales forecasting with seasonal patterns\n",
        "- **Methods**: Traditional ML, ARIMA/SARIMA, and Prophet models\n",
        "- **Outcome**: Actionable insights for business planning and inventory management\n",
        "\n",
        "### Key Features\n",
        "- ‚úÖ Dynamic path handling for portability across environments\n",
        "- ‚úÖ Comprehensive exploratory data analysis with visualizations\n",
        "- ‚úÖ Advanced preprocessing with reusable functions\n",
        "- ‚úÖ Time-based feature engineering\n",
        "- ‚úÖ Multiple forecasting models comparison\n",
        "- ‚úÖ Interactive visualizations\n",
        "- ‚úÖ Business insights and recommendations"
      ],
      "metadata": {
        "id": "project-overview"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup and Data Loading\n",
        "\n",
        "### Import Required Libraries\n",
        "Setting up the environment with all necessary libraries for data analysis, visualization, and modeling."
      ],
      "metadata": {
        "id": "setup-section"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Core data manipulation libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Machine learning libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Time series libraries\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from prophet import Prophet\n",
        "\n",
        "# Date and time handling\n",
        "from datetime import datetime, timedelta\n",
        "import dateutil.parser\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dynamic Path Configuration\n",
        "\n",
        "This section implements portable path handling that works across different environments (local, Colab, Jupyter, etc.)"
      ],
      "metadata": {
        "id": "path-config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_data_paths():\n",
        "    \"\"\"\n",
        "    Setup data paths dynamically based on the environment.\n",
        "    Works in Google Colab, local Jupyter, and other environments.\n",
        "    \"\"\"\n",
        "    # Check if running in Google Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        # Running in Google Colab\n",
        "        datasets_path = Path('/content/drive/MyDrive/AIML/CAPSTONE/datasets')\n",
        "        if not datasets_path.exists():\n",
        "            # Fallback to current directory if drive not mounted\n",
        "            datasets_path = Path('./datasets')\n",
        "        print(f\"üîó Google Colab environment detected\")\n",
        "    except ImportError:\n",
        "        # Running locally or in other environments\n",
        "        datasets_path = Path('./datasets')\n",
        "        print(f\"üíª Local environment detected\")\n",
        "    \n",
        "    # Create datasets directory if it doesn't exist\n",
        "    datasets_path.mkdir(exist_ok=True)\n",
        "    \n",
        "    print(f\"üìÅ Data path: {datasets_path.absolute()}\")\n",
        "    return datasets_path\n",
        "\n",
        "# Setup paths\n",
        "datasets_path = setup_data_paths()\n",
        "\n",
        "# Define file paths\n",
        "files = {\n",
        "    'sales': datasets_path / 'sales.csv',\n",
        "    'items': datasets_path / 'items.csv',\n",
        "    'restaurants': datasets_path / 'restaurants.csv'\n",
        "}\n",
        "\n",
        "print(f\"\\nüìã Expected files:\")\n",
        "for name, path in files.items():\n",
        "    exists = \"‚úÖ\" if path.exists() else \"‚ùå\"\n",
        "    print(f\"  {exists} {name}: {path.name}\")"
      ],
      "metadata": {
        "id": "path-setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading and Initial Inspection\n",
        "\n",
        "Loading the three main datasets and performing initial data quality checks."
      ],
      "metadata": {
        "id": "data-loading"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets(files):\n",
        "    \"\"\"\n",
        "    Load all datasets with error handling and initial validation.\n",
        "    \"\"\"\n",
        "    datasets = {}\n",
        "    \n",
        "    for name, file_path in files.items():\n",
        "        try:\n",
        "            if file_path.exists():\n",
        "                df = pd.read_csv(file_path)\n",
        "                datasets[name] = df\n",
        "                print(f\"‚úÖ Loaded {name}: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
        "            else:\n",
        "                print(f\"‚ùå File not found: {file_path}\")\n",
        "                # Create sample data if files don't exist\n",
        "                print(f\"üîß Creating sample {name} data...\")\n",
        "                if name == 'sales':\n",
        "                    datasets[name] = create_sample_sales_data()\n",
        "                elif name == 'items':\n",
        "                    datasets[name] = create_sample_items_data()\n",
        "                elif name == 'restaurants':\n",
        "                    datasets[name] = create_sample_restaurants_data()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading {name}: {e}\")\n",
        "    \n",
        "    return datasets\n",
        "\n",
        "def create_sample_sales_data():\n",
        "    \"\"\"Create sample sales data if file doesn't exist\"\"\"\n",
        "    dates = pd.date_range('2020-01-01', '2021-12-31', freq='D')\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    data = []\n",
        "    for i in range(10000):  # Reduced sample size\n",
        "        data.append({\n",
        "            'date': np.random.choice(dates).strftime('%Y-%m-%d'),\n",
        "            'item_id': np.random.randint(1, 51),\n",
        "            'store_id': np.random.randint(1, 6),\n",
        "            'item_count': np.random.choice([1, 2, 3, 4], p=[0.6, 0.25, 0.1, 0.05])\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def create_sample_items_data():\n",
        "    \"\"\"Create sample items data if file doesn't exist\"\"\"\n",
        "    categories = ['Main Course', 'Appetizer', 'Dessert', 'Beverage']\n",
        "    data = []\n",
        "    for i in range(1, 51):\n",
        "        data.append({\n",
        "            'id': i,\n",
        "            'name': f'Item {i}',\n",
        "            'category': np.random.choice(categories),\n",
        "            'price': round(np.random.uniform(5.99, 29.99), 2),\n",
        "            'kcal': np.random.randint(50, 800)\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def create_sample_restaurants_data():\n",
        "    \"\"\"Create sample restaurants data if file doesn't exist\"\"\"\n",
        "    return pd.DataFrame({\n",
        "        'id': [1, 2, 3, 4, 5],\n",
        "        'name': ['Restaurant A', 'Restaurant B', 'Restaurant C', 'Restaurant D', 'Restaurant E'],\n",
        "        'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n",
        "        'state': ['NY', 'CA', 'IL', 'TX', 'AZ']\n",
        "    })\n",
        "\n",
        "# Load datasets\n",
        "datasets = load_datasets(files)\n",
        "sales_df = datasets['sales']\n",
        "items_df = datasets['items']\n",
        "restaurants_df = datasets['restaurants']\n",
        "\n",
        "print(f\"\\nüìä Dataset Summary:\")\n",
        "print(f\"  Sales transactions: {len(sales_df):,}\")\n",
        "print(f\"  Unique items: {len(items_df):,}\")\n",
        "print(f\"  Restaurant locations: {len(restaurants_df):,}\")"
      ],
      "metadata": {
        "id": "load-data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Quality Assessment and Preprocessing\n",
        "\n",
        "### Initial Data Quality Check\n",
        "Comprehensive assessment of data quality, missing values, and data types."
      ],
      "metadata": {
        "id": "data-quality"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_data_quality(df, name):\n",
        "    \"\"\"\n",
        "    Comprehensive data quality analysis function.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"üìã DATA QUALITY REPORT: {name.upper()}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Basic info\n",
        "    print(f\"\\nüìè Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
        "    \n",
        "    # Data types\n",
        "    print(f\"\\nüè∑Ô∏è  Data Types:\")\n",
        "    for col, dtype in df.dtypes.items():\n",
        "        print(f\"   {col}: {dtype}\")\n",
        "    \n",
        "    # Missing values\n",
        "    missing = df.isnull().sum()\n",
        "    print(f\"\\n‚ùì Missing Values:\")\n",
        "    if missing.sum() == 0:\n",
        "        print(f\"   ‚úÖ No missing values found!\")\n",
        "    else:\n",
        "        for col, count in missing[missing > 0].items():\n",
        "            pct = (count / len(df)) * 100\n",
        "            print(f\"   {col}: {count:,} ({pct:.1f}%)\")\n",
        "    \n",
        "    # Duplicates\n",
        "    duplicates = df.duplicated().sum()\n",
        "    print(f\"\\nüîÑ Duplicate Rows: {duplicates:,}\")\n",
        "    \n",
        "    # Preview\n",
        "    print(f\"\\nüëÄ Sample Data:\")\n",
        "    display(df.head())\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Analyze each dataset\n",
        "sales_df = analyze_data_quality(sales_df, \"Sales\")\n",
        "items_df = analyze_data_quality(items_df, \"Items\")\n",
        "restaurants_df = analyze_data_quality(restaurants_df, \"Restaurants\")"
      ],
      "metadata": {
        "id": "data-quality-check"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing Functions\n",
        "\n",
        "Reusable functions for data cleaning, type conversion, and preprocessing."
      ],
      "metadata": {
        "id": "preprocessing-functions"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sales_data(sales_df):\n",
        "    \"\"\"\n",
        "    Comprehensive preprocessing for sales data.\n",
        "    \"\"\"\n",
        "    print(\"üîß Preprocessing sales data...\")\n",
        "    \n",
        "    # Make a copy to avoid modifying original\n",
        "    df = sales_df.copy()\n",
        "    \n",
        "    # Convert date column to datetime\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    print(f\"   ‚úÖ Converted date column to datetime\")\n",
        "    \n",
        "    # Sort by date\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "    print(f\"   ‚úÖ Sorted by date\")\n",
        "    \n",
        "    # Handle missing values\n",
        "    initial_rows = len(df)\n",
        "    df = df.dropna()\n",
        "    if initial_rows != len(df):\n",
        "        print(f\"   ‚úÖ Removed {initial_rows - len(df)} rows with missing values\")\n",
        "    \n",
        "    # Remove duplicates\n",
        "    initial_rows = len(df)\n",
        "    df = df.drop_duplicates()\n",
        "    if initial_rows != len(df):\n",
        "        print(f\"   ‚úÖ Removed {initial_rows - len(df)} duplicate rows\")\n",
        "    \n",
        "    # Validate data ranges\n",
        "    df = df[df['item_count'] > 0]  # Remove zero or negative counts\n",
        "    print(f\"   ‚úÖ Validated data ranges\")\n",
        "    \n",
        "    print(f\"   üìä Final shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "def detect_outliers(df, column, method='iqr'):\n",
        "    \"\"\"\n",
        "    Detect outliers using IQR or Z-score method.\n",
        "    \"\"\"\n",
        "    if method == 'iqr':\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
        "    else:  # z-score\n",
        "        z_scores = np.abs((df[column] - df[column].mean()) / df[column].std())\n",
        "        outliers = df[z_scores > 3]\n",
        "    \n",
        "    return outliers\n",
        "\n",
        "def handle_outliers(df, column, method='cap'):\n",
        "    \"\"\"\n",
        "    Handle outliers by capping or removing.\n",
        "    \"\"\"\n",
        "    outliers_before = len(detect_outliers(df, column))\n",
        "    \n",
        "    if method == 'cap':\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n",
        "    elif method == 'remove':\n",
        "        outliers = detect_outliers(df, column)\n",
        "        df = df.drop(outliers.index)\n",
        "    \n",
        "    outliers_after = len(detect_outliers(df, column))\n",
        "    print(f\"   üìä {column}: {outliers_before} ‚Üí {outliers_after} outliers\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Apply preprocessing\n",
        "sales_clean = preprocess_sales_data(sales_df)\n",
        "\n",
        "print(f\"\\n‚ú® Preprocessing complete!\")\n",
        "print(f\"   Original sales data: {len(sales_df):,} rows\")\n",
        "print(f\"   Cleaned sales data: {len(sales_clean):,} rows\")\n",
        "print(f\"   Date range: {sales_clean['date'].min()} to {sales_clean['date'].max()}\")"
      ],
      "metadata": {
        "id": "preprocessing"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Integration\n",
        "\n",
        "Merging datasets to create a comprehensive view for analysis."
      ],
      "metadata": {
        "id": "data-integration"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_datasets(sales_df, items_df, restaurants_df):\n",
        "    \"\"\"\n",
        "    Merge all datasets into a comprehensive dataframe.\n",
        "    \"\"\"\n",
        "    print(\"üîó Merging datasets...\")\n",
        "    \n",
        "    # Merge sales with items\n",
        "    merged = sales_df.merge(items_df, left_on='item_id', right_on='id', how='left', suffixes=('', '_item'))\n",
        "    print(f\"   ‚úÖ Merged with items: {len(merged):,} rows\")\n",
        "    \n",
        "    # Merge with restaurants\n",
        "    merged = merged.merge(restaurants_df, left_on='store_id', right_on='id', how='left', suffixes=('', '_restaurant'))\n",
        "    print(f\"   ‚úÖ Merged with restaurants: {len(merged):,} rows\")\n",
        "    \n",
        "    # Calculate total sales amount\n",
        "    merged['total_sales'] = merged['item_count'] * merged['price']\n",
        "    print(f\"   ‚úÖ Calculated total sales\")\n",
        "    \n",
        "    # Clean up duplicate columns\n",
        "    columns_to_drop = ['id_item', 'id_restaurant']\n",
        "    merged = merged.drop(columns=[col for col in columns_to_drop if col in merged.columns])\n",
        "    \n",
        "    # Rename columns for clarity\n",
        "    merged = merged.rename(columns={\n",
        "        'name': 'item_name',\n",
        "        'name_restaurant': 'restaurant_name'\n",
        "    })\n",
        "    \n",
        "    print(f\"   üìä Final merged dataset: {merged.shape}\")\n",
        "    print(f\"   üí∞ Total sales value: ${merged['total_sales'].sum():,.2f}\")\n",
        "    \n",
        "    return merged\n",
        "\n",
        "# Merge datasets\n",
        "merged_df = merge_datasets(sales_clean, items_df, restaurants_df)\n",
        "\n",
        "# Display sample of merged data\n",
        "print(f\"\\nüëÄ Sample of merged data:\")\n",
        "display(merged_df.head())\n",
        "\n",
        "print(f\"\\nüìã Column summary:\")\n",
        "for col in merged_df.columns:\n",
        "    print(f\"   {col}: {merged_df[col].dtype}\")"
      ],
      "metadata": {
        "id": "data-merge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Comprehensive Exploratory Data Analysis (EDA)\n",
        "\n",
        "### Sales Distribution and Trends Analysis\n",
        "Analyzing patterns in sales data across time, categories, and locations."
      ],
      "metadata": {
        "id": "eda-section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sales_overview_plots(df):\n",
        "    \"\"\"\n",
        "    Create comprehensive overview plots for sales data.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Sales Data Overview', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Daily sales trend\n",
        "    daily_sales = df.groupby('date')['total_sales'].sum().reset_index()\n",
        "    axes[0, 0].plot(daily_sales['date'], daily_sales['total_sales'], color='steelblue', alpha=0.7)\n",
        "    axes[0, 0].set_title('Daily Sales Trend', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Date')\n",
        "    axes[0, 0].set_ylabel('Total Sales ($)')\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 2. Sales by category\n",
        "    category_sales = df.groupby('category')['total_sales'].sum().sort_values(ascending=True)\n",
        "    axes[0, 1].barh(category_sales.index, category_sales.values, color='lightcoral')\n",
        "    axes[0, 1].set_title('Sales by Item Category', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Total Sales ($)')\n",
        "    \n",
        "    # 3. Sales by restaurant\n",
        "    restaurant_sales = df.groupby('restaurant_name')['total_sales'].sum().sort_values(ascending=True)\n",
        "    axes[1, 0].barh(restaurant_sales.index, restaurant_sales.values, color='lightgreen')\n",
        "    axes[1, 0].set_title('Sales by Restaurant', fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Total Sales ($)')\n",
        "    \n",
        "    # 4. Item count distribution\n",
        "    axes[1, 1].hist(df['item_count'], bins=20, color='gold', alpha=0.7, edgecolor='black')\n",
        "    axes[1, 1].set_title('Distribution of Item Count per Transaction', fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Item Count')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create overview plots\n",
        "create_sales_overview_plots(merged_df)\n",
        "\n",
        "# Print key statistics\n",
        "print(\"üìä KEY STATISTICS\")\n",
        "print(f\"{'='*40}\")\n",
        "print(f\"Total Sales Revenue: ${merged_df['total_sales'].sum():,.2f}\")\n",
        "print(f\"Average Transaction Value: ${merged_df['total_sales'].mean():.2f}\")\n",
        "print(f\"Median Transaction Value: ${merged_df['total_sales'].median():.2f}\")\n",
        "print(f\"Number of Transactions: {len(merged_df):,}\")\n",
        "print(f\"Date Range: {merged_df['date'].min().strftime('%Y-%m-%d')} to {merged_df['date'].max().strftime('%Y-%m-%d')}\")\n",
        "print(f\"Unique Items Sold: {merged_df['item_id'].nunique()}\")\n",
        "print(f\"Active Restaurants: {merged_df['store_id'].nunique()}\")"
      ],
      "metadata": {
        "id": "sales-overview"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time Series Analysis\n",
        "\n",
        "Deep dive into temporal patterns and seasonality in sales data."
      ],
      "metadata": {
        "id": "time-series-analysis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_temporal_patterns(df):\n",
        "    \"\"\"\n",
        "    Analyze temporal patterns in sales data.\n",
        "    \"\"\"\n",
        "    # Create time-based features\n",
        "    df_temp = df.copy()\n",
        "    df_temp['year'] = df_temp['date'].dt.year\n",
        "    df_temp['month'] = df_temp['date'].dt.month\n",
        "    df_temp['day_of_week'] = df_temp['date'].dt.dayofweek\n",
        "    df_temp['day_of_year'] = df_temp['date'].dt.dayofyear\n",
        "    df_temp['week_of_year'] = df_temp['date'].dt.isocalendar().week\n",
        "    df_temp['is_weekend'] = df_temp['day_of_week'].isin([5, 6])\n",
        "    \n",
        "    # Day names for better visualization\n",
        "    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('Temporal Pattern Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Monthly sales pattern\n",
        "    monthly_sales = df_temp.groupby('month')['total_sales'].sum()\n",
        "    axes[0, 0].bar(monthly_sales.index, monthly_sales.values, color='skyblue')\n",
        "    axes[0, 0].set_title('Sales by Month', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Month')\n",
        "    axes[0, 0].set_ylabel('Total Sales ($)')\n",
        "    axes[0, 0].set_xticks(range(1, 13))\n",
        "    axes[0, 0].set_xticklabels(month_names, rotation=45)\n",
        "    \n",
        "    # 2. Day of week pattern\n",
        "    dow_sales = df_temp.groupby('day_of_week')['total_sales'].sum()\n",
        "    axes[0, 1].bar(dow_sales.index, dow_sales.values, color='lightcoral')\n",
        "    axes[0, 1].set_title('Sales by Day of Week', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Day of Week')\n",
        "    axes[0, 1].set_ylabel('Total Sales ($)')\n",
        "    axes[0, 1].set_xticks(range(7))\n",
        "    axes[0, 1].set_xticklabels(day_names, rotation=45)\n",
        "    \n",
        "    # 3. Weekend vs Weekday\n",
        "    weekend_sales = df_temp.groupby('is_weekend')['total_sales'].sum()\n",
        "    weekend_labels = ['Weekday', 'Weekend']\n",
        "    axes[0, 2].pie(weekend_sales.values, labels=weekend_labels, autopct='%1.1f%%', \n",
        "                   colors=['lightblue', 'lightgreen'])\n",
        "    axes[0, 2].set_title('Weekend vs Weekday Sales', fontweight='bold')\n",
        "    \n",
        "    # 4. Weekly rolling average\n",
        "    daily_sales = df_temp.groupby('date')['total_sales'].sum().reset_index()\n",
        "    daily_sales['rolling_7d'] = daily_sales['total_sales'].rolling(window=7).mean()\n",
        "    axes[1, 0].plot(daily_sales['date'], daily_sales['total_sales'], alpha=0.3, color='gray', label='Daily')\n",
        "    axes[1, 0].plot(daily_sales['date'], daily_sales['rolling_7d'], color='red', linewidth=2, label='7-day average')\n",
        "    axes[1, 0].set_title('Sales Trend with 7-Day Moving Average', fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Date')\n",
        "    axes[1, 0].set_ylabel('Total Sales ($)')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 5. Hourly pattern (if time data available, simulate it)\n",
        "    # Simulate hourly patterns for demonstration\n",
        "    np.random.seed(42)\n",
        "    hours = range(24)\n",
        "    hourly_pattern = [0.2, 0.1, 0.05, 0.05, 0.1, 0.3, 0.8, 1.2, 1.0, 0.9, \n",
        "                     1.1, 1.5, 1.8, 1.6, 1.4, 1.2, 1.0, 1.1, 1.3, 1.4, 1.2, 0.8, 0.5, 0.3]\n",
        "    axes[1, 1].plot(hours, hourly_pattern, marker='o', color='orange', linewidth=2)\n",
        "    axes[1, 1].set_title('Simulated Hourly Sales Pattern', fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Hour of Day')\n",
        "    axes[1, 1].set_ylabel('Relative Sales Volume')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 6. Seasonal decomposition preview\n",
        "    if len(daily_sales) > 365:  # Need enough data for seasonal decomposition\n",
        "        # Simple trend analysis\n",
        "        axes[1, 2].scatter(daily_sales.index, daily_sales['total_sales'], alpha=0.5, color='purple')\n",
        "        # Fit trend line\n",
        "        z = np.polyfit(daily_sales.index, daily_sales['total_sales'], 1)\n",
        "        p = np.poly1d(z)\n",
        "        axes[1, 2].plot(daily_sales.index, p(daily_sales.index), \"r--\", alpha=0.8, linewidth=2)\n",
        "        axes[1, 2].set_title('Sales Trend Analysis', fontweight='bold')\n",
        "        axes[1, 2].set_xlabel('Days from Start')\n",
        "        axes[1, 2].set_ylabel('Total Sales ($)')\n",
        "    else:\n",
        "        axes[1, 2].text(0.5, 0.5, 'Insufficient data\\nfor trend analysis', \n",
        "                       ha='center', va='center', transform=axes[1, 2].transAxes)\n",
        "        axes[1, 2].set_title('Trend Analysis (Insufficient Data)', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return df_temp\n",
        "\n",
        "# Analyze temporal patterns\n",
        "df_with_time_features = analyze_temporal_patterns(merged_df)\n",
        "\n",
        "# Statistical summary of temporal patterns\n",
        "print(\"\\nüìà TEMPORAL PATTERN INSIGHTS\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "# Weekend vs weekday analysis\n",
        "weekend_avg = df_with_time_features[df_with_time_features['is_weekend']]['total_sales'].mean()\n",
        "weekday_avg = df_with_time_features[~df_with_time_features['is_weekend']]['total_sales'].mean()\n",
        "print(f\"Average Weekend Transaction: ${weekend_avg:.2f}\")\n",
        "print(f\"Average Weekday Transaction: ${weekday_avg:.2f}\")\n",
        "print(f\"Weekend Premium: {((weekend_avg / weekday_avg) - 1) * 100:.1f}%\")\n",
        "\n",
        "# Seasonal analysis\n",
        "monthly_avg = df_with_time_features.groupby('month')['total_sales'].mean()\n",
        "best_month = monthly_avg.idxmax()\n",
        "worst_month = monthly_avg.idxmin()\n",
        "print(f\"\\nBest performing month: {best_month} (${monthly_avg[best_month]:.2f} avg)\")\n",
        "print(f\"Worst performing month: {worst_month} (${monthly_avg[worst_month]:.2f} avg)\")"
      ],
      "metadata": {
        "id": "temporal-analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation Analysis\n",
        "\n",
        "Understanding relationships between different variables using correlation heatmaps and analysis."
      ],
      "metadata": {
        "id": "correlation-analysis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_correlation_analysis(df):\n",
        "    \"\"\"\n",
        "    Create comprehensive correlation analysis with heatmaps.\n",
        "    \"\"\"\n",
        "    # Prepare data for correlation analysis\n",
        "    df_corr = df.copy()\n",
        "    \n",
        "    # Create encoded categorical variables for correlation\n",
        "    df_corr['category_encoded'] = pd.Categorical(df_corr['category']).codes\n",
        "    df_corr['restaurant_encoded'] = pd.Categorical(df_corr['restaurant_name']).codes\n",
        "    df_corr['state_encoded'] = pd.Categorical(df_corr['state']).codes\n",
        "    \n",
        "    # Select numerical columns for correlation\n",
        "    numerical_cols = ['item_count', 'price', 'kcal', 'total_sales', \n",
        "                     'year', 'month', 'day_of_week', 'day_of_year',\n",
        "                     'category_encoded', 'restaurant_encoded', 'state_encoded']\n",
        "    \n",
        "    # Filter columns that exist in the dataframe\n",
        "    available_cols = [col for col in numerical_cols if col in df_corr.columns]\n",
        "    \n",
        "    corr_matrix = df_corr[available_cols].corr()\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "    \n",
        "    # 1. Full correlation heatmap\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, \n",
        "                square=True, ax=axes[0], fmt='.2f', cbar_kws={'shrink': 0.8})\n",
        "    axes[0].set_title('Feature Correlation Heatmap', fontweight='bold', pad=20)\n",
        "    axes[0].tick_params(axis='both', rotation=45)\n",
        "    \n",
        "    # 2. Sales-focused correlation\n",
        "    sales_corr = corr_matrix['total_sales'].sort_values(ascending=False)\n",
        "    colors = ['green' if x > 0 else 'red' for x in sales_corr.values]\n",
        "    axes[1].barh(range(len(sales_corr)), sales_corr.values, color=colors, alpha=0.7)\n",
        "    axes[1].set_yticks(range(len(sales_corr)))\n",
        "    axes[1].set_yticklabels(sales_corr.index)\n",
        "    axes[1].set_xlabel('Correlation with Total Sales')\n",
        "    axes[1].set_title('Features Correlation with Sales', fontweight='bold', pad=20)\n",
        "    axes[1].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "    \n",
        "    # Add correlation values as text\n",
        "    for i, v in enumerate(sales_corr.values):\n",
        "        axes[1].text(v + (0.02 if v >= 0 else -0.02), i, f'{v:.2f}', \n",
        "                    va='center', ha='left' if v >= 0 else 'right')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return corr_matrix, sales_corr\n",
        "\n",
        "# Create correlation analysis\n",
        "corr_matrix, sales_correlations = create_correlation_analysis(df_with_time_features)\n",
        "\n",
        "print(\"\\nüîó CORRELATION INSIGHTS\")\n",
        "print(f\"{'='*40}\")\n",
        "print(\"Top positive correlations with sales:\")\n",
        "positive_corr = sales_correlations[sales_correlations > 0].sort_values(ascending=False)\n",
        "for feature, corr in positive_corr.head(5).items():\n",
        "    if feature != 'total_sales':  # Exclude self-correlation\n",
        "        print(f\"  {feature}: {corr:.3f}\")\n",
        "\n",
        "print(\"\\nTop negative correlations with sales:\")\n",
        "negative_corr = sales_correlations[sales_correlations < 0].sort_values()\n",
        "for feature, corr in negative_corr.head(3).items():\n",
        "    print(f\"  {feature}: {corr:.3f}\")"
      ],
      "metadata": {
        "id": "correlation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Advanced Feature Engineering\n",
        "\n",
        "### Time-Based Feature Engineering\n",
        "Creating sophisticated time-based features for improved forecasting accuracy."
      ],
      "metadata": {
        "id": "feature-engineering"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_advanced_time_features(df):\n",
        "    \"\"\"\n",
        "    Create comprehensive time-based features for forecasting.\n",
        "    \"\"\"\n",
        "    print(\"üîß Creating advanced time-based features...\")\n",
        "    \n",
        "    df_features = df.copy()\n",
        "    \n",
        "    # Basic time features\n",
        "    df_features['year'] = df_features['date'].dt.year\n",
        "    df_features['month'] = df_features['date'].dt.month\n",
        "    df_features['day'] = df_features['date'].dt.day\n",
        "    df_features['day_of_week'] = df_features['date'].dt.dayofweek\n",
        "    df_features['day_of_year'] = df_features['date'].dt.dayofyear\n",
        "    df_features['week_of_year'] = df_features['date'].dt.isocalendar().week\n",
        "    df_features['quarter'] = df_features['date'].dt.quarter\n",
        "    \n",
        "    # Advanced time features\n",
        "    df_features['is_weekend'] = df_features['day_of_week'].isin([5, 6]).astype(int)\n",
        "    df_features['is_month_start'] = df_features['date'].dt.is_month_start.astype(int)\n",
        "    df_features['is_month_end'] = df_features['date'].dt.is_month_end.astype(int)\n",
        "    df_features['is_quarter_start'] = df_features['date'].dt.is_quarter_start.astype(int)\n",
        "    df_features['is_quarter_end'] = df_features['date'].dt.is_quarter_end.astype(int)\n",
        "    \n",
        "    # Cyclical features (sin/cos encoding for cyclical nature)\n",
        "    df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
        "    df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
        "    df_features['day_of_week_sin'] = np.sin(2 * np.pi * df_features['day_of_week'] / 7)\n",
        "    df_features['day_of_week_cos'] = np.cos(2 * np.pi * df_features['day_of_week'] / 7)\n",
        "    df_features['day_of_year_sin'] = np.sin(2 * np.pi * df_features['day_of_year'] / 365)\n",
        "    df_features['day_of_year_cos'] = np.cos(2 * np.pi * df_features['day_of_year'] / 365)\n",
        "    \n",
        "    # Holiday features (simplified)\n",
        "    df_features['is_holiday'] = 0\n",
        "    # New Year's period\n",
        "    df_features.loc[(df_features['month'] == 1) & (df_features['day'] <= 5), 'is_holiday'] = 1\n",
        "    # Christmas period\n",
        "    df_features.loc[(df_features['month'] == 12) & (df_features['day'] >= 20), 'is_holiday'] = 1\n",
        "    \n",
        "    print(f\"   ‚úÖ Created {len([col for col in df_features.columns if col not in df.columns])} new time features\")\n",
        "    \n",
        "    return df_features\n",
        "\n",
        "def create_lag_features(df, target_col='total_sales', lags=[1, 3, 7, 14, 30]):\n",
        "    \"\"\"\n",
        "    Create lag features for time series forecasting.\n",
        "    \"\"\"\n",
        "    print(f\"üîß Creating lag features for {target_col}...\")\n",
        "    \n",
        "    # Aggregate daily sales first\n",
        "    daily_sales = df.groupby('date')[target_col].sum().reset_index()\n",
        "    daily_sales = daily_sales.sort_values('date')\n",
        "    \n",
        "    # Create lag features\n",
        "    for lag in lags:\n",
        "        daily_sales[f'{target_col}_lag_{lag}'] = daily_sales[target_col].shift(lag)\n",
        "    \n",
        "    # Rolling window features\n",
        "    for window in [3, 7, 14, 30]:\n",
        "        daily_sales[f'{target_col}_rolling_mean_{window}'] = daily_sales[target_col].rolling(window=window).mean()\n",
        "        daily_sales[f'{target_col}_rolling_std_{window}'] = daily_sales[target_col].rolling(window=window).std()\n",
        "        daily_sales[f'{target_col}_rolling_min_{window}'] = daily_sales[target_col].rolling(window=window).min()\n",
        "        daily_sales[f'{target_col}_rolling_max_{window}'] = daily_sales[target_col].rolling(window=window).max()\n",
        "    \n",
        "    # Exponential moving averages\n",
        "    for alpha in [0.1, 0.3, 0.5]:\n",
        "        daily_sales[f'{target_col}_ema_{alpha}'] = daily_sales[target_col].ewm(alpha=alpha).mean()\n",
        "    \n",
        "    print(f\"   ‚úÖ Created lag and rolling features\")\n",
        "    \n",
        "    return daily_sales\n",
        "\n",
        "def create_aggregated_features(df):\n",
        "    \"\"\"\n",
        "    Create aggregated features for different time windows.\n",
        "    \"\"\"\n",
        "    print(\"üîß Creating aggregated features...\")\n",
        "    \n",
        "    # Weekly aggregations\n",
        "    df['week_start'] = df['date'] - pd.to_timedelta(df['date'].dt.dayofweek, unit='d')\n",
        "    weekly_agg = df.groupby(['week_start', 'store_id']).agg({\n",
        "        'total_sales': ['sum', 'mean', 'count'],\n",
        "        'item_count': ['sum', 'mean'],\n",
        "        'price': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Flatten column names\n",
        "    weekly_agg.columns = ['_'.join(col).strip() if col[1] else col[0] for col in weekly_agg.columns.values]\n",
        "    weekly_agg = weekly_agg.rename(columns={'week_start_': 'week_start', 'store_id_': 'store_id'})\n",
        "    \n",
        "    # Monthly aggregations\n",
        "    df['month_start'] = df['date'].dt.to_period('M').dt.start_time\n",
        "    monthly_agg = df.groupby(['month_start', 'store_id']).agg({\n",
        "        'total_sales': ['sum', 'mean', 'count'],\n",
        "        'item_count': ['sum', 'mean'],\n",
        "        'price': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Flatten column names\n",
        "    monthly_agg.columns = ['_'.join(col).strip() if col[1] else col[0] for col in monthly_agg.columns.values]\n",
        "    monthly_agg = monthly_agg.rename(columns={'month_start_': 'month_start', 'store_id_': 'store_id'})\n",
        "    \n",
        "    print(f\"   ‚úÖ Created weekly aggregations: {weekly_agg.shape}\")\n",
        "    print(f\"   ‚úÖ Created monthly aggregations: {monthly_agg.shape}\")\n",
        "    \n",
        "    return weekly_agg, monthly_agg\n",
        "\n",
        "# Apply feature engineering\n",
        "df_with_features = create_advanced_time_features(df_with_time_features)\n",
        "daily_sales_with_lags = create_lag_features(df_with_features)\n",
        "weekly_agg, monthly_agg = create_aggregated_features(df_with_features)\n",
        "\n",
        "print(f\"\\n‚ú® Feature Engineering Summary:\")\n",
        "print(f\"   Original features: {len(df_with_time_features.columns)}\")\n",
        "print(f\"   Enhanced features: {len(df_with_features.columns)}\")\n",
        "print(f\"   Daily sales with lags: {daily_sales_with_lags.shape}\")\n",
        "print(f\"   Weekly aggregations: {weekly_agg.shape}\")\n",
        "print(f\"   Monthly aggregations: {monthly_agg.shape}\")\n",
        "\n",
        "# Display sample of enhanced features\n",
        "print(f\"\\nüëÄ Sample of time-enhanced features:\")\n",
        "feature_cols = ['date', 'total_sales', 'month_sin', 'month_cos', 'day_of_week_sin', \n",
        "               'is_weekend', 'is_holiday', 'quarter']\n",
        "available_features = [col for col in feature_cols if col in df_with_features.columns]\n",
        "display(df_with_features[available_features].head())"
      ],
      "metadata": {
        "id": "feature-engineering-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Machine Learning Models\n",
        "\n",
        "### Traditional Machine Learning Approach\n",
        "Implementing baseline models with cross-validation and performance evaluation."
      ],
      "metadata": {
        "id": "ml-models"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_ml_data(daily_sales_df, target_col='total_sales'):\n",
        "    \"\"\"\n",
        "    Prepare data for machine learning models.\n",
        "    \"\"\"\n",
        "    print(\"üîß Preparing data for ML models...\")\n",
        "    \n",
        "    # Remove rows with NaN values (from lag features)\n",
        "    ml_data = daily_sales_df.dropna().copy()\n",
        "    \n",
        "    # Feature columns (exclude target and date)\n",
        "    feature_cols = [col for col in ml_data.columns if col not in ['date', target_col]]\n",
        "    \n",
        "    X = ml_data[feature_cols]\n",
        "    y = ml_data[target_col]\n",
        "    \n",
        "    # Time-based split (last 20% for testing)\n",
        "    split_index = int(len(ml_data) * 0.8)\n",
        "    \n",
        "    X_train = X.iloc[:split_index]\n",
        "    X_test = X.iloc[split_index:]\n",
        "    y_train = y.iloc[:split_index]\n",
        "    y_test = y.iloc[split_index:]\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    print(f\"   ‚úÖ Training set: {X_train.shape}\")\n",
        "    print(f\"   ‚úÖ Test set: {X_test.shape}\")\n",
        "    print(f\"   ‚úÖ Features: {len(feature_cols)}\")\n",
        "    \n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, feature_cols, ml_data\n",
        "\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    \"\"\"\n",
        "    Comprehensive model evaluation.\n",
        "    \"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    # MAPE (Mean Absolute Percentage Error)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    \n",
        "    print(f\"\\nüìä {model_name} Performance:\")\n",
        "    print(f\"   RMSE: ${rmse:,.2f}\")\n",
        "    print(f\"   MAE:  ${mae:,.2f}\")\n",
        "    print(f\"   R¬≤:   {r2:.3f}\")\n",
        "    print(f\"   MAPE: {mape:.2f}%\")\n",
        "    \n",
        "    return {'rmse': rmse, 'mae': mae, 'r2': r2, 'mape': mape}\n",
        "\n",
        "def train_ml_models(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate multiple ML models.\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Training machine learning models...\")\n",
        "    \n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    trained_models = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nüîÑ Training {name}...\")\n",
        "        \n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "        \n",
        "        # Evaluate\n",
        "        results[name] = evaluate_model(y_test, y_pred, name)\n",
        "        results[name]['predictions'] = y_pred\n",
        "        trained_models[name] = model\n",
        "    \n",
        "    return results, trained_models\n",
        "\n",
        "# Prepare data and train models\n",
        "if len(daily_sales_with_lags) > 50:  # Ensure we have enough data\n",
        "    X_train, X_test, y_train, y_test, scaler, feature_cols, ml_data = prepare_ml_data(daily_sales_with_lags)\n",
        "    ml_results, trained_models = train_ml_models(X_train, X_test, y_train, y_test)\n",
        "    \n",
        "    # Create comparison plot\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Model comparison\n",
        "    metrics = ['rmse', 'mae', 'r2', 'mape']\n",
        "    model_names = list(ml_results.keys())\n",
        "    \n",
        "    metric_data = {metric: [ml_results[model][metric] for model in model_names] for metric in metrics}\n",
        "    \n",
        "    # RMSE comparison\n",
        "    axes[0].bar(model_names, [ml_results[model]['rmse'] for model in model_names], \n",
        "               color=['skyblue', 'lightcoral'])\n",
        "    axes[0].set_title('Model Comparison - RMSE', fontweight='bold')\n",
        "    axes[0].set_ylabel('RMSE ($)')\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Prediction vs Actual for best model\n",
        "    best_model = min(ml_results.keys(), key=lambda x: ml_results[x]['rmse'])\n",
        "    y_pred_best = ml_results[best_model]['predictions']\n",
        "    \n",
        "    axes[1].scatter(y_test, y_pred_best, alpha=0.6, color='green')\n",
        "    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "    axes[1].set_xlabel('Actual Sales ($)')\n",
        "    axes[1].set_ylabel('Predicted Sales ($)')\n",
        "    axes[1].set_title(f'Predictions vs Actual - {best_model}', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nüèÜ Best Model: {best_model} (RMSE: ${ml_results[best_model]['rmse']:,.2f})\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Insufficient data for ML model training\")\n",
        "    ml_results = {}\n",
        "    trained_models = {}"
      ],
      "metadata": {
        "id": "ml-models-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Advanced Time Series Forecasting\n",
        "\n",
        "### Time Series Stationarity and Preparation\n",
        "Analyzing stationarity and preparing data for ARIMA/SARIMA models."
      ],
      "metadata": {
        "id": "timeseries-prep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_stationarity(timeseries, title):\n",
        "    \"\"\"\n",
        "    Check stationarity using Augmented Dickey-Fuller test.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìä Stationarity Test: {title}\")\n",
        "    print('=' * 50)\n",
        "    \n",
        "    # Perform Augmented Dickey-Fuller test\n",
        "    result = adfuller(timeseries, autolag='AIC')\n",
        "    \n",
        "    print(f'ADF Statistic: {result[0]:.6f}')\n",
        "    print(f'p-value: {result[1]:.6f}')\n",
        "    print('Critical Values:')\n",
        "    for key, value in result[4].items():\n",
        "        print(f'\\t{key}: {value:.3f}')\n",
        "    \n",
        "    if result[1] <= 0.05:\n",
        "        print(\"‚úÖ Series is stationary (reject null hypothesis)\")\n",
        "        is_stationary = True\n",
        "    else:\n",
        "        print(\"‚ùå Series is non-stationary (fail to reject null hypothesis)\")\n",
        "        is_stationary = False\n",
        "    \n",
        "    return is_stationary\n",
        "\n",
        "def prepare_timeseries_data(df, target_col='total_sales'):\n",
        "    \"\"\"\n",
        "    Prepare time series data for ARIMA/SARIMA modeling.\n",
        "    \"\"\"\n",
        "    print(\"üîß Preparing time series data...\")\n",
        "    \n",
        "    # Create daily time series\n",
        "    daily_ts = df.groupby('date')[target_col].sum().reset_index()\n",
        "    daily_ts = daily_ts.set_index('date').sort_index()\n",
        "    \n",
        "    # Fill missing dates with interpolation\n",
        "    daily_ts = daily_ts.asfreq('D')\n",
        "    daily_ts[target_col] = daily_ts[target_col].interpolate(method='linear')\n",
        "    \n",
        "    print(f\"   ‚úÖ Time series shape: {daily_ts.shape}\")\n",
        "    print(f\"   ‚úÖ Date range: {daily_ts.index.min()} to {daily_ts.index.max()}\")\n",
        "    \n",
        "    return daily_ts\n",
        "\n",
        "def seasonal_decomposition_analysis(ts, period=30):\n",
        "    \"\"\"\n",
        "    Perform seasonal decomposition analysis.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç Seasonal Decomposition Analysis (period={period})\")\n",
        "    \n",
        "    if len(ts) >= 2 * period:\n",
        "        decomposition = seasonal_decompose(ts.squeeze(), model='additive', period=period)\n",
        "        \n",
        "        fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
        "        \n",
        "        decomposition.observed.plot(ax=axes[0], title='Original Time Series', color='blue')\n",
        "        decomposition.trend.plot(ax=axes[1], title='Trend Component', color='green')\n",
        "        decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component', color='orange')\n",
        "        decomposition.resid.plot(ax=axes[3], title='Residual Component', color='red')\n",
        "        \n",
        "        for ax in axes:\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "            ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return decomposition\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è Insufficient data for seasonal decomposition (need at least {2*period} points)\")\n",
        "        return None\n",
        "\n",
        "# Prepare time series data\n",
        "if len(df_with_features) > 0:\n",
        "    ts_data = prepare_timeseries_data(df_with_features)\n",
        "    \n",
        "    # Check stationarity\n",
        "    is_stationary = check_stationarity(ts_data['total_sales'].dropna(), \"Original Series\")\n",
        "    \n",
        "    # Seasonal decomposition\n",
        "    decomposition = seasonal_decomposition_analysis(ts_data['total_sales'].dropna())\n",
        "    \n",
        "    # If non-stationary, try differencing\n",
        "    if not is_stationary:\n",
        "        ts_data['total_sales_diff'] = ts_data['total_sales'].diff()\n",
        "        is_stationary_diff = check_stationarity(ts_data['total_sales_diff'].dropna(), \"First Difference\")\n",
        "        \n",
        "        if not is_stationary_diff:\n",
        "            ts_data['total_sales_diff2'] = ts_data['total_sales_diff'].diff()\n",
        "            check_stationarity(ts_data['total_sales_diff2'].dropna(), \"Second Difference\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No data available for time series analysis\")\n",
        "    ts_data = None"
      ],
      "metadata": {
        "id": "ts-prep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ARIMA and SARIMA Models\n",
        "\n",
        "Implementing traditional statistical time series models."
      ],
      "metadata": {
        "id": "arima-sarima"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_arima_model(ts, order=(1, 1, 1)):\n",
        "    \"\"\"\n",
        "    Fit ARIMA model to time series data.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîÑ Fitting ARIMA{order} model...\")\n",
        "    \n",
        "    try:\n",
        "        # Split data for training and testing\n",
        "        train_size = int(len(ts) * 0.8)\n",
        "        train, test = ts[:train_size], ts[train_size:]\n",
        "        \n",
        "        # Fit ARIMA model\n",
        "        model = ARIMA(train, order=order)\n",
        "        fitted_model = model.fit()\n",
        "        \n",
        "        # Make predictions\n",
        "        forecast = fitted_model.forecast(steps=len(test))\n",
        "        \n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(test, forecast)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(test, forecast)\n",
        "        \n",
        "        print(f\"   ‚úÖ ARIMA{order} - RMSE: ${rmse:.2f}, MAE: ${mae:.2f}\")\n",
        "        print(f\"   üìä AIC: {fitted_model.aic:.2f}, BIC: {fitted_model.bic:.2f}\")\n",
        "        \n",
        "        return {\n",
        "            'model': fitted_model,\n",
        "            'forecast': forecast,\n",
        "            'train': train,\n",
        "            'test': test,\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'aic': fitted_model.aic,\n",
        "            'bic': fitted_model.bic\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error fitting ARIMA{order}: {e}\")\n",
        "        return None\n",
        "\n",
        "def fit_sarima_model(ts, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)):\n",
        "    \"\"\"\n",
        "    Fit SARIMA model to time series data.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîÑ Fitting SARIMA{order}x{seasonal_order} model...\")\n",
        "    \n",
        "    try:\n",
        "        # Split data for training and testing\n",
        "        train_size = int(len(ts) * 0.8)\n",
        "        train, test = ts[:train_size], ts[train_size:]\n",
        "        \n",
        "        # Fit SARIMA model\n",
        "        model = SARIMAX(train, order=order, seasonal_order=seasonal_order)\n",
        "        fitted_model = model.fit(disp=False)\n",
        "        \n",
        "        # Make predictions\n",
        "        forecast = fitted_model.forecast(steps=len(test))\n",
        "        \n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(test, forecast)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(test, forecast)\n",
        "        \n",
        "        print(f\"   ‚úÖ SARIMA{order}x{seasonal_order} - RMSE: ${rmse:.2f}, MAE: ${mae:.2f}\")\n",
        "        print(f\"   üìä AIC: {fitted_model.aic:.2f}, BIC: {fitted_model.bic:.2f}\")\n",
        "        \n",
        "        return {\n",
        "            'model': fitted_model,\n",
        "            'forecast': forecast,\n",
        "            'train': train,\n",
        "            'test': test,\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'aic': fitted_model.aic,\n",
        "            'bic': fitted_model.bic\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error fitting SARIMA{order}x{seasonal_order}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Fit ARIMA and SARIMA models\n",
        "arima_sarima_results = {}\n",
        "\n",
        "if ts_data is not None and len(ts_data['total_sales'].dropna()) > 50:\n",
        "    ts_clean = ts_data['total_sales'].dropna()\n",
        "    \n",
        "    # Try different ARIMA configurations\n",
        "    arima_configs = [(1, 1, 1), (2, 1, 1), (1, 1, 2), (2, 1, 2)]\n",
        "    \n",
        "    for order in arima_configs:\n",
        "        result = fit_arima_model(ts_clean, order)\n",
        "        if result:\n",
        "            arima_sarima_results[f'ARIMA{order}'] = result\n",
        "    \n",
        "    # Try SARIMA with different configurations\n",
        "    sarima_configs = [\n",
        "        ((1, 1, 1), (1, 1, 1, 7)),   # Weekly seasonality\n",
        "        ((1, 1, 1), (1, 1, 1, 30)),  # Monthly seasonality\n",
        "    ]\n",
        "    \n",
        "    for order, seasonal_order in sarima_configs:\n",
        "        if len(ts_clean) > 2 * seasonal_order[3]:  # Need enough data for seasonality\n",
        "            result = fit_sarima_model(ts_clean, order, seasonal_order)\n",
        "            if result:\n",
        "                arima_sarima_results[f'SARIMA{order}x{seasonal_order}'] = result\n",
        "    \n",
        "    # Find best model\n",
        "    if arima_sarima_results:\n",
        "        best_model_name = min(arima_sarima_results.keys(), \n",
        "                             key=lambda x: arima_sarima_results[x]['rmse'])\n",
        "        best_result = arima_sarima_results[best_model_name]\n",
        "        \n",
        "        print(f\"\\nüèÜ Best ARIMA/SARIMA Model: {best_model_name}\")\n",
        "        print(f\"   RMSE: ${best_result['rmse']:.2f}\")\n",
        "        print(f\"   MAE: ${best_result['mae']:.2f}\")\n",
        "        print(f\"   AIC: {best_result['aic']:.2f}\")\n",
        "        \n",
        "        # Plot best model results\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        \n",
        "        # Plot training data\n",
        "        plt.plot(best_result['train'].index, best_result['train'].values, \n",
        "                label='Training Data', color='blue', alpha=0.7)\n",
        "        \n",
        "        # Plot test data\n",
        "        plt.plot(best_result['test'].index, best_result['test'].values, \n",
        "                label='Actual Test Data', color='green')\n",
        "        \n",
        "        # Plot forecast\n",
        "        plt.plot(best_result['test'].index, best_result['forecast'], \n",
        "                label=f'{best_model_name} Forecast', color='red', linestyle='--')\n",
        "        \n",
        "        plt.title(f'{best_model_name} - Sales Forecasting', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Total Sales ($)')\n",
        "        plt.legend()\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Insufficient data for ARIMA/SARIMA modeling\")"
      ],
      "metadata": {
        "id": "arima-sarima-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prophet Model\n",
        "\n",
        "Implementing Facebook Prophet for robust time series forecasting with automatic seasonality detection."
      ],
      "metadata": {
        "id": "prophet-model"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_prophet_model(df, target_col='total_sales'):\n",
        "    \"\"\"\n",
        "    Fit Prophet model to time series data.\n",
        "    \"\"\"\n",
        "    print(\"\\nüîÆ Fitting Prophet model...\")\n",
        "    \n",
        "    try:\n",
        "        # Prepare data for Prophet (requires 'ds' and 'y' columns)\n",
        "        daily_sales = df.groupby('date')[target_col].sum().reset_index()\n",
        "        daily_sales.columns = ['ds', 'y']\n",
        "        \n",
        "        # Split data\n",
        "        train_size = int(len(daily_sales) * 0.8)\n",
        "        train_data = daily_sales[:train_size]\n",
        "        test_data = daily_sales[train_size:]\n",
        "        \n",
        "        # Initialize and fit Prophet model\n",
        "        model = Prophet(\n",
        "            yearly_seasonality=True,\n",
        "            weekly_seasonality=True,\n",
        "            daily_seasonality=False,\n",
        "            seasonality_mode='additive',\n",
        "            changepoint_prior_scale=0.05\n",
        "        )\n",
        "        \n",
        "        # Add custom seasonalities\n",
        "        model.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
        "        \n",
        "        # Fit the model\n",
        "        model.fit(train_data)\n",
        "        \n",
        "        # Create future dataframe for predictions\n",
        "        future = model.make_future_dataframe(periods=len(test_data))\n",
        "        \n",
        "        # Make predictions\n",
        "        forecast = model.predict(future)\n",
        "        \n",
        "        # Extract test predictions\n",
        "        test_forecast = forecast.tail(len(test_data))\n",
        "        \n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(test_data['y'], test_forecast['yhat'])\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(test_data['y'], test_forecast['yhat'])\n",
        "        r2 = r2_score(test_data['y'], test_forecast['yhat'])\n",
        "        \n",
        "        print(f\"   ‚úÖ Prophet Model Performance:\")\n",
        "        print(f\"      RMSE: ${rmse:.2f}\")\n",
        "        print(f\"      MAE:  ${mae:.2f}\")\n",
        "        print(f\"      R¬≤:   {r2:.3f}\")\n",
        "        \n",
        "        return {\n",
        "            'model': model,\n",
        "            'forecast': forecast,\n",
        "            'train_data': train_data,\n",
        "            'test_data': test_data,\n",
        "            'test_forecast': test_forecast,\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'r2': r2\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error fitting Prophet model: {e}\")\n",
        "        return None\n",
        "\n",
        "def plot_prophet_results(prophet_result):\n",
        "    \"\"\"\n",
        "    Create comprehensive Prophet results visualization.\n",
        "    \"\"\"\n",
        "    if prophet_result is None:\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Prophet Model Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Forecast plot\n",
        "    forecast = prophet_result['forecast']\n",
        "    train_data = prophet_result['train_data']\n",
        "    test_data = prophet_result['test_data']\n",
        "    \n",
        "    axes[0, 0].plot(train_data['ds'], train_data['y'], 'b-', label='Training Data', alpha=0.7)\n",
        "    axes[0, 0].plot(test_data['ds'], test_data['y'], 'g-', label='Actual Test Data', linewidth=2)\n",
        "    axes[0, 0].plot(forecast['ds'], forecast['yhat'], 'r--', label='Prophet Forecast', alpha=0.8)\n",
        "    axes[0, 0].fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], \n",
        "                           color='red', alpha=0.2, label='Confidence Interval')\n",
        "    axes[0, 0].set_title('Sales Forecast with Prophet', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Date')\n",
        "    axes[0, 0].set_ylabel('Total Sales ($)')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 2. Residuals plot\n",
        "    test_forecast = prophet_result['test_forecast']\n",
        "    residuals = test_data['y'].values - test_forecast['yhat'].values\n",
        "    axes[0, 1].scatter(test_forecast['yhat'], residuals, alpha=0.6)\n",
        "    axes[0, 1].axhline(y=0, color='red', linestyle='--')\n",
        "    axes[0, 1].set_title('Residuals Plot', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Predicted Values')\n",
        "    axes[0, 1].set_ylabel('Residuals')\n",
        "    \n",
        "    # 3. Weekly seasonality\n",
        "    weekly_seasonality = forecast[['ds', 'weekly']].copy()\n",
        "    weekly_seasonality['day_of_week'] = weekly_seasonality['ds'].dt.dayofweek\n",
        "    weekly_avg = weekly_seasonality.groupby('day_of_week')['weekly'].mean()\n",
        "    day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "    axes[1, 0].bar(range(7), weekly_avg.values, color='skyblue')\n",
        "    axes[1, 0].set_title('Weekly Seasonality', fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Day of Week')\n",
        "    axes[1, 0].set_ylabel('Effect on Sales')\n",
        "    axes[1, 0].set_xticks(range(7))\n",
        "    axes[1, 0].set_xticklabels(day_names)\n",
        "    \n",
        "    # 4. Yearly trend\n",
        "    axes[1, 1].plot(forecast['ds'], forecast['trend'], color='green', linewidth=2)\n",
        "    axes[1, 1].set_title('Trend Component', fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Date')\n",
        "    axes[1, 1].set_ylabel('Trend')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Fit Prophet model\n",
        "if len(df_with_features) > 0:\n",
        "    prophet_result = fit_prophet_model(df_with_features)\n",
        "    \n",
        "    if prophet_result:\n",
        "        # Plot Prophet results\n",
        "        plot_prophet_results(prophet_result)\n",
        "        \n",
        "        # Future forecasting\n",
        "        print(\"\\nüîÆ Creating future forecasts...\")\n",
        "        \n",
        "        # Create future dates (next 30 days)\n",
        "        future_30_days = prophet_result['model'].make_future_dataframe(periods=30)\n",
        "        future_forecast = prophet_result['model'].predict(future_30_days)\n",
        "        \n",
        "        # Plot future forecast\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        plt.plot(future_forecast['ds'], future_forecast['yhat'], 'b-', linewidth=2, label='Forecast')\n",
        "        plt.fill_between(future_forecast['ds'], future_forecast['yhat_lower'], \n",
        "                        future_forecast['yhat_upper'], alpha=0.3, label='Confidence Interval')\n",
        "        \n",
        "        # Highlight future period\n",
        "        future_start = prophet_result['test_data']['ds'].max()\n",
        "        plt.axvline(x=future_start, color='red', linestyle='--', alpha=0.7, label='Forecast Start')\n",
        "        \n",
        "        plt.title('30-Day Sales Forecast', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Predicted Sales ($)')\n",
        "        plt.legend()\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Print future forecast summary\n",
        "        future_period = future_forecast[future_forecast['ds'] > future_start]\n",
        "        avg_daily_forecast = future_period['yhat'].mean()\n",
        "        total_forecast = future_period['yhat'].sum()\n",
        "        \n",
        "        print(f\"   üìä 30-Day Forecast Summary:\")\n",
        "        print(f\"      Average daily sales: ${avg_daily_forecast:,.2f}\")\n",
        "        print(f\"      Total 30-day sales: ${total_forecast:,.2f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No data available for Prophet modeling\")\n",
        "    prophet_result = None"
      ],
      "metadata": {
        "id": "prophet-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Comparison and Performance Analysis\n",
        "\n",
        "### Comprehensive Model Comparison\n",
        "Comparing all models side-by-side to identify the best performing approach."
      ],
      "metadata": {
        "id": "model-comparison"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_comparison(ml_results, arima_sarima_results, prophet_result):\n",
        "    \"\"\"\n",
        "    Create comprehensive comparison of all models.\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä COMPREHENSIVE MODEL COMPARISON\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    comparison_data = []\n",
        "    \n",
        "    # Add ML models\n",
        "    for model_name, results in ml_results.items():\n",
        "        comparison_data.append({\n",
        "            'Model': model_name,\n",
        "            'Type': 'Machine Learning',\n",
        "            'RMSE': results['rmse'],\n",
        "            'MAE': results['mae'],\n",
        "            'R¬≤': results.get('r2', 'N/A'),\n",
        "            'MAPE': results.get('mape', 'N/A')\n",
        "        })\n",
        "    \n",
        "    # Add ARIMA/SARIMA models\n",
        "    for model_name, results in arima_sarima_results.items():\n",
        "        comparison_data.append({\n",
        "            'Model': model_name,\n",
        "            'Type': 'Time Series',\n",
        "            'RMSE': results['rmse'],\n",
        "            'MAE': results['mae'],\n",
        "            'R¬≤': 'N/A',\n",
        "            'MAPE': 'N/A'\n",
        "        })\n",
        "    \n",
        "    # Add Prophet model\n",
        "    if prophet_result:\n",
        "        comparison_data.append({\n",
        "            'Model': 'Prophet',\n",
        "            'Type': 'Time Series',\n",
        "            'RMSE': prophet_result['rmse'],\n",
        "            'MAE': prophet_result['mae'],\n",
        "            'R¬≤': prophet_result['r2'],\n",
        "            'MAPE': 'N/A'\n",
        "        })\n",
        "    \n",
        "    if comparison_data:\n",
        "        # Create comparison DataFrame\n",
        "        comparison_df = pd.DataFrame(comparison_data)\n",
        "        \n",
        "        # Display comparison table\n",
        "        print(\"\\nüìã Model Performance Summary:\")\n",
        "        display(comparison_df.round(3))\n",
        "        \n",
        "        # Find best model by RMSE\n",
        "        best_model_idx = comparison_df['RMSE'].idxmin()\n",
        "        best_model = comparison_df.loc[best_model_idx]\n",
        "        \n",
        "        print(f\"\\nüèÜ BEST PERFORMING MODEL\")\n",
        "        print(f\"   Model: {best_model['Model']}\")\n",
        "        print(f\"   Type: {best_model['Type']}\")\n",
        "        print(f\"   RMSE: ${best_model['RMSE']:,.2f}\")\n",
        "        print(f\"   MAE: ${best_model['MAE']:,.2f}\")\n",
        "        \n",
        "        # Create visualization\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "        \n",
        "        # RMSE comparison\n",
        "        colors = ['lightblue' if t == 'Machine Learning' else 'lightcoral' \n",
        "                 for t in comparison_df['Type']]\n",
        "        axes[0].bar(comparison_df['Model'], comparison_df['RMSE'], color=colors)\n",
        "        axes[0].set_title('Model Comparison - RMSE', fontweight='bold')\n",
        "        axes[0].set_ylabel('RMSE ($)')\n",
        "        axes[0].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # MAE comparison\n",
        "        axes[1].bar(comparison_df['Model'], comparison_df['MAE'], color=colors)\n",
        "        axes[1].set_title('Model Comparison - MAE', fontweight='bold')\n",
        "        axes[1].set_ylabel('MAE ($)')\n",
        "        axes[1].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Add legend\n",
        "        from matplotlib.patches import Patch\n",
        "        legend_elements = [Patch(facecolor='lightblue', label='Machine Learning'),\n",
        "                          Patch(facecolor='lightcoral', label='Time Series')]\n",
        "        axes[0].legend(handles=legend_elements)\n",
        "        axes[1].legend(handles=legend_elements)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return comparison_df, best_model\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è No models available for comparison\")\n",
        "        return None, None\n",
        "\n",
        "# Create model comparison\n",
        "comparison_df, best_model = create_model_comparison(ml_results, arima_sarima_results, prophet_result)\n",
        "\n",
        "# Model insights and recommendations\n",
        "if comparison_df is not None:\n",
        "    print(\"\\nüí° MODEL INSIGHTS & RECOMMENDATIONS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Analyze model types performance\n",
        "    ml_models = comparison_df[comparison_df['Type'] == 'Machine Learning']\n",
        "    ts_models = comparison_df[comparison_df['Type'] == 'Time Series']\n",
        "    \n",
        "    if len(ml_models) > 0 and len(ts_models) > 0:\n",
        "        ml_avg_rmse = ml_models['RMSE'].mean()\n",
        "        ts_avg_rmse = ts_models['RMSE'].mean()\n",
        "        \n",
        "        print(f\"Average RMSE - ML Models: ${ml_avg_rmse:,.2f}\")\n",
        "        print(f\"Average RMSE - Time Series Models: ${ts_avg_rmse:,.2f}\")\n",
        "        \n",
        "        if ml_avg_rmse < ts_avg_rmse:\n",
        "            print(\"\\nüéØ Machine Learning models generally outperform time series models\")\n",
        "            print(\"   This suggests strong feature relationships beyond temporal patterns\")\n",
        "        else:\n",
        "            print(\"\\nüéØ Time series models generally outperform machine learning models\")\n",
        "            print(\"   This suggests strong temporal dependencies in the data\")\n",
        "    \n",
        "    # Specific recommendations\n",
        "    print(\"\\nüìã RECOMMENDATIONS:\")\n",
        "    if best_model['Type'] == 'Machine Learning':\n",
        "        print(\"   ‚Ä¢ Use machine learning approach for short-term predictions\")\n",
        "        print(\"   ‚Ä¢ Consider ensemble methods combining ML and time series\")\n",
        "        print(\"   ‚Ä¢ Focus on feature engineering and external factors\")\n",
        "    else:\n",
        "        print(\"   ‚Ä¢ Use time series approach for medium to long-term forecasting\")\n",
        "        print(\"   ‚Ä¢ Consider Prophet for automatic seasonality detection\")\n",
        "        print(\"   ‚Ä¢ Monitor for structural breaks in time series\")\n",
        "    \n",
        "    print(\"   ‚Ä¢ Implement ensemble forecasting for robust predictions\")\n",
        "    print(\"   ‚Ä¢ Regular model retraining and validation\")\n",
        "    print(\"   ‚Ä¢ Monitor forecast accuracy in production\")"
      ],
      "metadata": {
        "id": "model-comparison-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Interactive Visualizations with Plotly\n",
        "\n",
        "### Enhanced Interactive Dashboard\n",
        "Creating interactive visualizations for better insights and presentation."
      ],
      "metadata": {
        "id": "interactive-viz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_interactive_sales_dashboard(df):\n",
        "    \"\"\"\n",
        "    Create interactive sales dashboard using Plotly.\n",
        "    \"\"\"\n",
        "    print(\"üé® Creating interactive sales dashboard...\")\n",
        "    \n",
        "    # Prepare daily sales data\n",
        "    daily_sales = df.groupby('date').agg({\n",
        "        'total_sales': 'sum',\n",
        "        'item_count': 'sum',\n",
        "        'store_id': 'nunique'\n",
        "    }).reset_index()\n",
        "    \n",
        "    daily_sales['avg_transaction'] = daily_sales['total_sales'] / daily_sales['item_count']\n",
        "    daily_sales['rolling_7d'] = daily_sales['total_sales'].rolling(window=7).mean()\n",
        "    \n",
        "    # Create subplot dashboard\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        subplot_titles=('Daily Sales Trend', 'Sales by Category', \n",
        "                       'Restaurant Performance', 'Monthly Sales Pattern'),\n",
        "        specs=[[{\"secondary_y\": True}, {\"type\": \"pie\"}],\n",
        "               [{\"type\": \"bar\"}, {\"type\": \"box\"}]]\n",
        "    )\n",
        "    \n",
        "    # 1. Daily sales trend with rolling average\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=daily_sales['date'], y=daily_sales['total_sales'],\n",
        "                  mode='lines', name='Daily Sales', opacity=0.6,\n",
        "                  line=dict(color='lightblue')),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Scatter(x=daily_sales['date'], y=daily_sales['rolling_7d'],\n",
        "                  mode='lines', name='7-Day Average',\n",
        "                  line=dict(color='red', width=2)),\n",
        "        row=1, col=1\n",
        "    )\n",
        "    \n",
        "    # 2. Sales by category (pie chart)\n",
        "    category_sales = df.groupby('category')['total_sales'].sum().reset_index()\n",
        "    fig.add_trace(\n",
        "        go.Pie(labels=category_sales['category'], values=category_sales['total_sales'],\n",
        "               name=\"Category Sales\"),\n",
        "        row=1, col=2\n",
        "    )\n",
        "    \n",
        "    # 3. Restaurant performance (bar chart)\n",
        "    restaurant_sales = df.groupby('restaurant_name')['total_sales'].sum().reset_index()\n",
        "    restaurant_sales = restaurant_sales.sort_values('total_sales', ascending=True)\n",
        "    \n",
        "    fig.add_trace(\n",
        "        go.Bar(x=restaurant_sales['total_sales'], y=restaurant_sales['restaurant_name'],\n",
        "               orientation='h', name=\"Restaurant Sales\",\n",
        "               marker_color='lightgreen'),\n",
        "        row=2, col=1\n",
        "    )\n",
        "    \n",
        "    # 4. Monthly sales distribution (box plot)\n",
        "    df_monthly = df.copy()\n",
        "    df_monthly['month_name'] = df_monthly['date'].dt.strftime('%B')\n",
        "    month_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
        "                  'July', 'August', 'September', 'October', 'November', 'December']\n",
        "    \n",
        "    for month in month_order:\n",
        "        if month in df_monthly['month_name'].values:\n",
        "            month_data = df_monthly[df_monthly['month_name'] == month]['total_sales']\n",
        "            fig.add_trace(\n",
        "                go.Box(y=month_data, name=month[:3], showlegend=False),\n",
        "                row=2, col=2\n",
        "            )\n",
        "    \n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title_text=\"Interactive Sales Analytics Dashboard\",\n",
        "        title_x=0.5,\n",
        "        title_font_size=16,\n",
        "        height=800,\n",
        "        showlegend=True\n",
        "    )\n",
        "    \n",
        "    # Update axes labels\n",
        "    fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Sales ($)\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Sales ($)\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"Restaurant\", row=2, col=1)\n",
        "    fig.update_yaxes(title_text=\"Sales ($)\", row=2, col=2)\n",
        "    \n",
        "    fig.show()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "def create_forecast_comparison_plot(comparison_df, ml_results, prophet_result):\n",
        "    \"\"\"\n",
        "    Create interactive forecast comparison visualization.\n",
        "    \"\"\"\n",
        "    if comparison_df is None:\n",
        "        print(\"‚ö†Ô∏è No comparison data available for visualization\")\n",
        "        return\n",
        "    \n",
        "    print(\"üé® Creating interactive forecast comparison...\")\n",
        "    \n",
        "    # Create interactive model performance comparison\n",
        "    fig = go.Figure()\n",
        "    \n",
        "    # Add RMSE bars\n",
        "    colors = ['lightblue' if t == 'Machine Learning' else 'lightcoral' \n",
        "             for t in comparison_df['Type']]\n",
        "    \n",
        "    fig.add_trace(go.Bar(\n",
        "        x=comparison_df['Model'],\n",
        "        y=comparison_df['RMSE'],\n",
        "        name='RMSE',\n",
        "        marker_color=colors,\n",
        "        text=[f'${x:,.0f}' for x in comparison_df['RMSE']],\n",
        "        textposition='auto',\n",
        "        hovertemplate='<b>%{x}</b><br>RMSE: $%{y:,.0f}<extra></extra>'\n",
        "    ))\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title='Interactive Model Performance Comparison',\n",
        "        title_x=0.5,\n",
        "        xaxis_title='Model',\n",
        "        yaxis_title='RMSE ($)',\n",
        "        hovermode='x unified',\n",
        "        height=500\n",
        "    )\n",
        "    \n",
        "    fig.show()\n",
        "    \n",
        "    # If Prophet results available, create forecast visualization\n",
        "    if prophet_result:\n",
        "        forecast = prophet_result['forecast']\n",
        "        train_data = prophet_result['train_data']\n",
        "        test_data = prophet_result['test_data']\n",
        "        \n",
        "        fig_forecast = go.Figure()\n",
        "        \n",
        "        # Add training data\n",
        "        fig_forecast.add_trace(go.Scatter(\n",
        "            x=train_data['ds'],\n",
        "            y=train_data['y'],\n",
        "            mode='lines',\n",
        "            name='Training Data',\n",
        "            line=dict(color='blue', width=2)\n",
        "        ))\n",
        "        \n",
        "        # Add test data\n",
        "        fig_forecast.add_trace(go.Scatter(\n",
        "            x=test_data['ds'],\n",
        "            y=test_data['y'],\n",
        "            mode='lines',\n",
        "            name='Actual Test Data',\n",
        "            line=dict(color='green', width=2)\n",
        "        ))\n",
        "        \n",
        "        # Add forecast\n",
        "        fig_forecast.add_trace(go.Scatter(\n",
        "            x=forecast['ds'],\n",
        "            y=forecast['yhat'],\n",
        "            mode='lines',\n",
        "            name='Prophet Forecast',\n",
        "            line=dict(color='red', width=2, dash='dash')\n",
        "        ))\n",
        "        \n",
        "        # Add confidence interval\n",
        "        fig_forecast.add_trace(go.Scatter(\n",
        "            x=forecast['ds'],\n",
        "            y=forecast['yhat_upper'],\n",
        "            mode='lines',\n",
        "            line=dict(width=0),\n",
        "            showlegend=False,\n",
        "            hoverinfo='skip'\n",
        "        ))\n",
        "        \n",
        "        fig_forecast.add_trace(go.Scatter(\n",
        "            x=forecast['ds'],\n",
        "            y=forecast['yhat_lower'],\n",
        "            mode='lines',\n",
        "            line=dict(width=0),\n",
        "            fillcolor='rgba(255,0,0,0.2)',\n",
        "            fill='tonexty',\n",
        "            name='Confidence Interval',\n",
        "            hoverinfo='skip'\n",
        "        ))\n",
        "        \n",
        "        fig_forecast.update_layout(\n",
        "            title='Interactive Sales Forecast (Prophet Model)',\n",
        "            title_x=0.5,\n",
        "            xaxis_title='Date',\n",
        "            yaxis_title='Sales ($)',\n",
        "            hovermode='x unified',\n",
        "            height=500\n",
        "        )\n",
        "        \n",
        "        fig_forecast.show()\n",
        "\n",
        "# Create interactive visualizations\n",
        "if len(df_with_features) > 0:\n",
        "    # Main dashboard\n",
        "    dashboard_fig = create_interactive_sales_dashboard(df_with_features)\n",
        "    \n",
        "    # Forecast comparison\n",
        "    create_forecast_comparison_plot(comparison_df, ml_results, prophet_result)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No data available for interactive visualizations\")"
      ],
      "metadata": {
        "id": "interactive-viz-code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Business Insights and Conclusions\n",
        "\n",
        "### Key Findings and Actionable Insights\n",
        "Summarizing the analysis results and providing business recommendations."
      ],
      "metadata": {
        "id": "conclusions"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_business_insights(df, comparison_df, best_model):\n",
        "    \"\"\"\n",
        "    Generate comprehensive business insights from the analysis.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üè¢ BUSINESS INSIGHTS & STRATEGIC RECOMMENDATIONS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Sales Performance Insights\n",
        "    print(\"\\nüìä SALES PERFORMANCE ANALYSIS\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    total_sales = df['total_sales'].sum()\n",
        "    avg_daily_sales = df.groupby('date')['total_sales'].sum().mean()\n",
        "    total_transactions = len(df)\n",
        "    avg_transaction_value = df['total_sales'].mean()\n",
        "    \n",
        "    print(f\"üí∞ Total Sales Revenue: ${total_sales:,.2f}\")\n",
        "    print(f\"üìÖ Average Daily Sales: ${avg_daily_sales:,.2f}\")\n",
        "    print(f\"üõí Total Transactions: {total_transactions:,}\")\n",
        "    print(f\"üí≥ Average Transaction Value: ${avg_transaction_value:.2f}\")\n",
        "    \n",
        "    # Temporal Insights\n",
        "    print(\"\\n‚è∞ TEMPORAL PATTERNS\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Day of week analysis\n",
        "    dow_sales = df.groupby(df['date'].dt.day_name())['total_sales'].sum()\n",
        "    best_day = dow_sales.idxmax()\n",
        "    worst_day = dow_sales.idxmin()\n",
        "    \n",
        "    print(f\"üèÜ Best performing day: {best_day} (${dow_sales[best_day]:,.2f})\")\n",
        "    print(f\"üìâ Lowest performing day: {worst_day} (${dow_sales[worst_day]:,.2f})\")\n",
        "    \n",
        "    # Weekend vs weekday\n",
        "    df_temp = df.copy()\n",
        "    df_temp['is_weekend'] = df_temp['date'].dt.dayofweek.isin([5, 6])\n",
        "    weekend_avg = df_temp[df_temp['is_weekend']]['total_sales'].mean()\n",
        "    weekday_avg = df_temp[~df_temp['is_weekend']]['total_sales'].mean()\n",
        "    weekend_premium = ((weekend_avg / weekday_avg) - 1) * 100\n",
        "    \n",
        "    print(f\"üéâ Weekend vs Weekday Premium: {weekend_premium:+.1f}%\")\n",
        "    \n",
        "    # Category Performance\n",
        "    print(\"\\nüçΩÔ∏è CATEGORY PERFORMANCE\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    category_performance = df.groupby('category').agg({\n",
        "        'total_sales': ['sum', 'mean'],\n",
        "        'item_count': 'sum'\n",
        "    }).round(2)\n",
        "    \n",
        "    category_performance.columns = ['Total_Sales', 'Avg_Transaction', 'Total_Items']\n",
        "    category_performance = category_performance.sort_values('Total_Sales', ascending=False)\n",
        "    \n",
        "    print(\"Top performing categories:\")\n",
        "    for i, (category, row) in enumerate(category_performance.head(3).iterrows()):\n",
        "        print(f\"  {i+1}. {category}: ${row['Total_Sales']:,.2f} total, ${row['Avg_Transaction']:.2f} avg\")\n",
        "    \n",
        "    # Restaurant Performance\n",
        "    print(\"\\nüè™ RESTAURANT PERFORMANCE\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    restaurant_performance = df.groupby('restaurant_name').agg({\n",
        "        'total_sales': ['sum', 'mean', 'count'],\n",
        "        'item_count': 'sum'\n",
        "    }).round(2)\n",
        "    \n",
        "    restaurant_performance.columns = ['Total_Sales', 'Avg_Transaction', 'Num_Transactions', 'Total_Items']\n",
        "    restaurant_performance = restaurant_performance.sort_values('Total_Sales', ascending=False)\n",
        "    \n",
        "    print(\"Top performing restaurants:\")\n",
        "    for i, (restaurant, row) in enumerate(restaurant_performance.head(3).iterrows()):\n",
        "        print(f\"  {i+1}. {restaurant}: ${row['Total_Sales']:,.2f} total, {row['Num_Transactions']} transactions\")\n",
        "    \n",
        "    # Model Performance Insights\n",
        "    if comparison_df is not None and best_model is not None:\n",
        "        print(\"\\nü§ñ FORECASTING MODEL INSIGHTS\")\n",
        "        print(\"-\" * 35)\n",
        "        \n",
        "        print(f\"üèÜ Best Model: {best_model['Model']} ({best_model['Type']})\")\n",
        "        print(f\"üìä Forecast Accuracy: RMSE ${best_model['RMSE']:,.2f}\")\n",
        "        \n",
        "        # Model type analysis\n",
        "        ml_models = comparison_df[comparison_df['Type'] == 'Machine Learning']\n",
        "        ts_models = comparison_df[comparison_df['Type'] == 'Time Series']\n",
        "        \n",
        "        if len(ml_models) > 0 and len(ts_models) > 0:\n",
        "            ml_avg_rmse = ml_models['RMSE'].mean()\n",
        "            ts_avg_rmse = ts_models['RMSE'].mean()\n",
        "            \n",
        "            if ml_avg_rmse < ts_avg_rmse:\n",
        "                better_approach = \"Machine Learning\"\n",
        "                performance_diff = ((ts_avg_rmse / ml_avg_rmse) - 1) * 100\n",
        "            else:\n",
        "                better_approach = \"Time Series\"\n",
        "                performance_diff = ((ml_avg_rmse / ts_avg_rmse) - 1) * 100\n",
        "            \n",
        "            print(f\"üìà {better_approach} models perform {performance_diff:.1f}% better on average\")\n",
        "    \n",
        "    # Strategic Recommendations\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üéØ STRATEGIC RECOMMENDATIONS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    print(\"\\nüí° OPERATIONAL RECOMMENDATIONS:\")\n",
        "    print(f\"   ‚Ä¢ Focus marketing efforts on {best_day}s for maximum impact\")\n",
        "    if weekend_premium > 0:\n",
        "        print(f\"   ‚Ä¢ Leverage weekend premium ({weekend_premium:.1f}%) with special promotions\")\n",
        "    else:\n",
        "        print(f\"   ‚Ä¢ Boost weekend sales with targeted promotions (currently {weekend_premium:.1f}% below weekdays)\")\n",
        "    \n",
        "    top_category = category_performance.index[0]\n",
        "    print(f\"   ‚Ä¢ Expand {top_category} offerings as the top-performing category\")\n",
        "    \n",
        "    top_restaurant = restaurant_performance.index[0]\n",
        "    print(f\"   ‚Ä¢ Replicate {top_restaurant}'s success strategies across other locations\")\n",
        "    \n",
        "    print(\"\\nüìä FORECASTING RECOMMENDATIONS:\")\n",
        "    if best_model is not None:\n",
        "        if best_model['Type'] == 'Machine Learning':\n",
        "            print(\"   ‚Ä¢ Implement ML-based forecasting for short-term planning\")\n",
        "            print(\"   ‚Ä¢ Focus on feature engineering and external data integration\")\n",
        "            print(\"   ‚Ä¢ Monitor key predictive features for early trend detection\")\n",
        "        else:\n",
        "            print(\"   ‚Ä¢ Use time series models for medium to long-term planning\")\n",
        "            print(\"   ‚Ä¢ Monitor seasonal patterns for inventory planning\")\n",
        "            print(\"   ‚Ä¢ Consider external factors affecting time series patterns\")\n",
        "    \n",
        "    print(\"   ‚Ä¢ Implement ensemble forecasting for robust predictions\")\n",
        "    print(\"   ‚Ä¢ Set up automated model retraining pipelines\")\n",
        "    print(\"   ‚Ä¢ Establish forecast accuracy monitoring and alerting\")\n",
        "    \n",
        "    print(\"\\nüöÄ GROWTH OPPORTUNITIES:\")\n",
        "    print(\"   ‚Ä¢ Optimize menu mix based on category performance analysis\")\n",
        "    print(\"   ‚Ä¢ Implement dynamic pricing during peak periods\")\n",
        "    print(\"   ‚Ä¢ Develop targeted customer acquisition strategies\")\n",
        "    print(\"   ‚Ä¢ Consider expansion in high-performing restaurant locations\")\n",
        "    print(\"   ‚Ä¢ Implement predictive inventory management\")\n",
        "    \n",
        "    print(\"\\nüìà NEXT STEPS:\")\n",
        "    print(\"   1. Deploy selected forecasting model to production\")\n",
        "    print(\"   2. Set up real-time data collection and monitoring\")\n",
        "    print(\"   3. Implement A/B testing for recommended strategies\")\n",
        "    print(\"   4. Establish KPIs and success metrics\")\n",
        "    print(\"   5. Schedule regular model performance reviews\")\n",
        "    \n",
        "    return {\n",
        "        'total_sales': total_sales,\n",
        "        'avg_daily_sales': avg_daily_sales,\n",
        "        'best_day': best_day,\n",
        "        'weekend_premium': weekend_premium,\n",
        "        'top_category': top_category,\n",
        "        'top_restaurant': top_restaurant\n",
        "    }\n",
        "\n",
        "# Generate comprehensive business insights\n",
        "if len(df_with_features) > 0:\n",
        "    business_insights = generate_business_insights(df_with_features, comparison_df, best_model)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No data available for business insights generation\")\n",
        "    \n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nThis comprehensive sales forecasting analysis provides:\")\n",
        "print(\"‚Ä¢ Advanced time series forecasting capabilities\")\n",
        "print(\"‚Ä¢ Multiple model comparison and validation\")\n",
        "print(\"‚Ä¢ Interactive visualizations for stakeholder presentation\")\n",
        "print(\"‚Ä¢ Actionable business insights and recommendations\")\n",
        "print(\"‚Ä¢ Portable code that works across different environments\")\n",
        "print(\"\\nThe notebook is now ready for deployment and further customization!\")"
      ],
      "metadata": {
        "id": "business-insights"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}